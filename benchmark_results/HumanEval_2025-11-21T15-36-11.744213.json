{
  "benchmark_name": "HumanEval",
  "score": 82.99953063653372,
  "metric": "pass@1",
  "timestamp": "2025-11-21T15:36:11.744213",
  "model_name": "MONSTERDOG",
  "model_version": "V_OMEGA_\u221e",
  "total_tasks": 164,
  "completed_tasks": 100,
  "execution_time_seconds": 1.0148286819458008,
  "metadata": {
    "type": "coding",
    "leaderboard_url": "https://paperswithcode.com/sota/code-generation-on-humaneval",
    "world_record": 92.0
  }
}