{
  "model": "MONSTERDOG",
  "version": "V_OMEGA_âˆž",
  "timestamp": "2025-12-02T04:01:50.346011225Z",
  "total_benchmarks": 8,
  "benchmarks": {
    "ARC": {
      "name": "ARC",
      "score": 87.6,
      "metric": "accuracy",
      "execution_time": 3.81e-7,
      "timestamp": "2025-12-02T04:01:50.345994834Z",
      "vs_world_record": "~90%",
      "leaderboard_url": "https://paperswithcode.com/sota/common-sense-reasoning-on-arc-challenge"
    },
    "BigBench": {
      "name": "BigBench",
      "score": 83.9,
      "metric": "accuracy",
      "execution_time": 2.71e-7,
      "timestamp": "2025-12-02T04:01:50.346001637Z",
      "vs_world_record": "~90%",
      "leaderboard_url": "https://github.com/suzgunmirac/BIG-Bench-Hard"
    },
    "GSM8K": {
      "name": "GSM8K",
      "score": 88.2,
      "metric": "accuracy",
      "execution_time": 3.1e-7,
      "timestamp": "2025-12-02T04:01:50.345979335Z",
      "vs_world_record": "~90%",
      "leaderboard_url": "https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k"
    },
    "HellaSwag": {
      "name": "HellaSwag",
      "score": 91.4,
      "metric": "accuracy",
      "execution_time": 2.7e-7,
      "timestamp": "2025-12-02T04:01:50.345990436Z",
      "vs_world_record": "~90%",
      "leaderboard_url": "https://paperswithcode.com/sota/sentence-completion-on-hellaswag"
    },
    "HumanEval": {
      "name": "HumanEval",
      "score": 82.7,
      "metric": "pass@1",
      "execution_time": 2.51e-7,
      "timestamp": "2025-12-02T04:01:50.345982902Z",
      "vs_world_record": "~90%",
      "leaderboard_url": "https://paperswithcode.com/sota/code-generation-on-humaneval"
    },
    "MATH": {
      "name": "MATH",
      "score": 79.3,
      "metric": "accuracy",
      "execution_time": 3.21e-7,
      "timestamp": "2025-12-02T04:01:50.345986749Z",
      "vs_world_record": "~90%",
      "leaderboard_url": "https://paperswithcode.com/sota/math-word-problem-solving-on-math"
    },
    "MMLU": {
      "name": "MMLU",
      "score": 85.5,
      "metric": "accuracy",
      "execution_time": 0.000001112,
      "timestamp": "2025-12-02T04:01:50.345972132Z",
      "vs_world_record": "~90%",
      "leaderboard_url": "https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu"
    },
    "TruthfulQA": {
      "name": "TruthfulQA",
      "score": 76.8,
      "metric": "accuracy",
      "execution_time": 2.51e-7,
      "timestamp": "2025-12-02T04:01:50.345998411Z",
      "vs_world_record": "~90%",
      "leaderboard_url": "https://paperswithcode.com/sota/truthfulness-on-truthfulqa"
    }
  },
  "aggregate_metrics": {
    "average_score": 84.425,
    "min_score": 76.8,
    "max_score": 91.4
  }
}