name: ğŸ† MONSTERDOG Benchmark Suite

on:
  # Run weekly on Sundays at midnight UTC
  schedule:
    - cron: '0 0 * * 0'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      benchmark_mode:
        description: 'Benchmark execution mode'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - quick
          - priority
  
  # Run on push to main for testing
  push:
    branches:
      - main
    paths:
      - 'src/benchmarks/**'
      - 'pkg/benchmarks/**'
      - '.github/workflows/benchmarks.yml'

jobs:
  run-benchmarks:
    name: Execute Official Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - name: ğŸ›¸ Checkout MONSTERDOG Repository
        uses: actions/checkout@v4
      
      - name: ğŸ Setup Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      
      - name: ğŸ“¦ Install Python Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy aiofiles
      
      - name: ğŸ”§ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: ğŸš€ Execute MONSTERDOG Benchmark Orchestrator
        id: benchmarks
        run: |
          echo "ğŸ† Starting MONSTERDOG Benchmark Suite..."
          python3 src/benchmarks/benchmark_orchestrator.py
        continue-on-error: true
      
      - name: ğŸ“Š Display Benchmark Summary
        if: always()
        run: |
          if [ -f benchmark_results/BENCHMARK_SUMMARY.json ]; then
            echo "ğŸ“Š BENCHMARK SUMMARY:"
            cat benchmark_results/BENCHMARK_SUMMARY.json | python3 -m json.tool
          else
            echo "âš ï¸ No benchmark summary found"
          fi
      
      - name: ğŸ’¾ Upload Benchmark Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: monsterdog-benchmark-results-${{ github.run_number }}
          path: benchmark_results/
          retention-days: 90
      
      - name: ğŸ“ˆ Create Benchmark Report
        if: always()
        run: |
          cat << 'EOF' > benchmark_report.md
          # ğŸ† MONSTERDOG Benchmark Results
          
          **Run Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Run Number**: ${{ github.run_number }}
          **Commit**: ${{ github.sha }}
          
          ## Summary
          
          EOF
          
          if [ -f benchmark_results/BENCHMARK_SUMMARY.json ]; then
            echo "âœ… Benchmarks completed successfully" >> benchmark_report.md
            echo "" >> benchmark_report.md
            echo "### Results" >> benchmark_report.md
            echo "" >> benchmark_report.md
            echo '```json' >> benchmark_report.md
            cat benchmark_results/BENCHMARK_SUMMARY.json >> benchmark_report.md
            echo '```' >> benchmark_report.md
          else
            echo "âš ï¸ Benchmark execution encountered issues" >> benchmark_report.md
          fi
          
          cat benchmark_report.md
      
      - name: ğŸ“¤ Upload Benchmark Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report-${{ github.run_number }}
          path: benchmark_report.md
          retention-days: 90
      
      - name: ğŸŒŸ Post Results Summary (on main branch)
        if: github.ref == 'refs/heads/main' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let summary = '# ğŸ† MONSTERDOG Benchmark Execution\n\n';
            
            try {
              const summaryData = JSON.parse(fs.readFileSync('benchmark_results/BENCHMARK_SUMMARY.json', 'utf8'));
              
              summary += `**Model**: ${summaryData.model} ${summaryData.version}\n`;
              summary += `**Timestamp**: ${summaryData.timestamp}\n`;
              summary += `**Total Benchmarks**: ${summaryData.total_benchmarks}\n\n`;
              
              summary += '## Benchmark Scores\n\n';
              summary += '| Benchmark | Score | Metric | vs World Record |\n';
              summary += '|-----------|-------|--------|----------------|\n';
              
              for (const [name, data] of Object.entries(summaryData.benchmarks)) {
                summary += `| ${name} | ${data.score.toFixed(2)} | ${data.metric} | ${data.vs_world_record} |\n`;
              }
              
              summary += `\n**Average Score**: ${summaryData.aggregate_metrics.average_score.toFixed(2)}%\n`;
              summary += '\nğŸŒ MONSTERDOG ready for world leaderboards!\n';
              
            } catch (error) {
              summary += 'âš ï¸ Error reading benchmark results\n';
              summary += `Error: ${error.message}\n`;
            }
            
            await core.summary.addRaw(summary).write();

  analyze-performance:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: run-benchmarks
    if: always()
    
    steps:
      - name: ğŸ›¸ Checkout Repository
        uses: actions/checkout@v4
      
      - name: ğŸ“¥ Download Benchmark Results
        uses: actions/download-artifact@v4
        with:
          pattern: monsterdog-benchmark-results-*
          path: ./results
          merge-multiple: true
      
      - name: ğŸ“Š Analyze Performance Trends
        run: |
          echo "ğŸ” Analyzing MONSTERDOG performance trends..."
          
          if [ -f results/BENCHMARK_SUMMARY.json ]; then
            echo "âœ… Latest benchmark results available"
            
            # Extract key metrics
            python3 -c "
          import json
          import sys
          
          try:
              with open('results/BENCHMARK_SUMMARY.json', 'r') as f:
                  data = json.load(f)
              
              print('\\nğŸ¯ PERFORMANCE HIGHLIGHTS:')
              print(f'   Model: {data[\"model\"]} {data[\"version\"]}')
              print(f'   Benchmarks Run: {data[\"total_benchmarks\"]}')
              
              if 'aggregate_metrics' in data:
                  avg = data['aggregate_metrics'].get('average_score', 0)
                  print(f'   Average Score: {avg:.2f}%')
              
              print('\\nğŸ† TOP PERFORMERS:')
              scores = [(name, info['score']) for name, info in data.get('benchmarks', {}).items()]
              scores.sort(key=lambda x: x[1], reverse=True)
              
              for i, (name, score) in enumerate(scores[:3], 1):
                  print(f'   {i}. {name}: {score:.2f}%')
              
              print('\\nâœ¨ MONSTERDOG Performance Analysis Complete\\n')
              
          except Exception as e:
              print(f'âš ï¸ Error analyzing results: {e}', file=sys.stderr)
              sys.exit(1)
          "
          else
            echo "âš ï¸ No benchmark results found for analysis"
          fi
      
      - name: ğŸ–ï¸ Badge Update Preparation
        run: |
          echo "Preparing badge data for README updates..."
          # Future enhancement: Update README badges with latest scores

  notify-completion:
    name: Notification
    runs-on: ubuntu-latest
    needs: [run-benchmarks, analyze-performance]
    if: always()
    
    steps:
      - name: ğŸ‰ Benchmark Completion Status
        run: |
          echo "ğŸ† MONSTERDOG Benchmark Suite Execution Complete!"
          echo "âš¡ï¸ FULLTRUTL AGENTIC MODE: âœ…"
          echo "ğŸŒ Results ready for world leaderboard submission"
          echo ""
          echo "Next steps:"
          echo "1. Review benchmark results in artifacts"
          echo "2. Verify scores against baselines"
          echo "3. Submit to official leaderboards"
          echo "4. Update MONSTERDOG documentation"
          echo ""
          echo "ğŸ”¥ MONSTERDOG - Making AI Consciousness Measurable ğŸ”¥"
